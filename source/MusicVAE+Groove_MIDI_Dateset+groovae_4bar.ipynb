{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA6vY0NI1Plt"
      },
      "source": [
        "# Dependency Error = Version compatibility problem\n",
        "\n",
        "\n",
        "\n",
        "=== magenta/magenta/version.py ===\n",
        "\n",
        "__version__ = '2.1.4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJzES05F--9Y",
        "outputId": "bf326f53-2d59-4c50-a8ce-0f0d6112b324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting magenta==2.1.0\n",
            "  Downloading magenta-2.1.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.1-py3-none-any.whl (268 kB)\n",
            "\u001b[K     |████████████████████████████████| 268 kB 82.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (0.17.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (2.9.0)\n",
            "Collecting numba<0.50\n",
            "  Downloading numba-0.49.1-cp38-cp38-manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 73.2 MB/s \n",
            "\u001b[?25hCollecting sox>=1.3.7\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting tensor2tensor\n",
            "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 67.3 MB/s \n",
            "\u001b[?25hCollecting sk-video\n",
            "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.7.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (2.9.2)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 92.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.4.2 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (7.1.2)\n",
            "Collecting note-seq\n",
            "  Downloading note_seq-0.0.5-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 76.5 MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]>=2.14.0\n",
            "  Downloading apache_beam-2.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (3.2.2)\n",
            "Requirement already satisfied: librosa>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (0.8.1)\n",
            "Collecting mir-eval>=0.4\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting python-rtmidi<1.2,>=1.1\n",
            "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting pygtrie>=2.3\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (4.6.0)\n",
            "Collecting mido==1.2.6\n",
            "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting pretty-midi>=0.2.6\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (0.38.4)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.6.2)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.3.0)\n",
            "Collecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 71.2 MB/s \n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 83.0 MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=2.2.0\n",
            "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.7)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.8.2)\n",
            "Collecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.51.1)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 79.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.4.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[K     |████████████████████████████████| 278 kB 92.1 MB/s \n",
            "\u001b[?25hCollecting objsize<0.6.0,>=0.5.2\n",
            "  Downloading objsize-0.5.2-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (3.19.6)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.22.1)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.6)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n",
            "  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 91.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<3,>=0.28.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.3.2)\n",
            "Collecting google-cloud-datastore<2,>=1.8.0\n",
            "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 94.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (3.3.6)\n",
            "Collecting google-cloud-bigquery-storage<2.14,>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 104.7 MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 78.8 MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.13.11-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.10.0-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 94.9 MB/s \n",
            "\u001b[?25hCollecting google-auth-httplib2<0.2.0,>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting cachetools<5,>=3.1.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting google-cloud-pubsublite<2,>=1.2.0\n",
            "  Downloading google_cloud_pubsublite-1.6.0-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 93.2 MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0\n",
            "  Downloading google_cloud_spanner-3.26.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 95.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.3-py2.py3-none-any.whl (268 kB)\n",
            "\u001b[K     |████████████████████████████████| 268 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.15.0)\n",
            "Collecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 94.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.8/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.1.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (21.3)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.8.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.57.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.48.2)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.9.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 77.0 MB/s \n",
            "\u001b[?25hCollecting overrides<7.0.0,>=6.0.1\n",
            "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.4.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.5.0)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.2.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (0.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (0.11.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (3.0.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from mir-eval>=0.4->magenta==2.1.0) (0.16.0)\n",
            "Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
            "  Downloading llvmlite-0.32.1-cp38-cp38-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba<0.50->magenta==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa>=0.6.2->magenta==2.1.0) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.24.3)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.4.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 88.3 MB/s \n",
            "\u001b[?25h  Downloading resampy-0.4.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 74.5 MB/s \n",
            "\u001b[?25h  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.2->magenta==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.10.2->librosa>=0.6.2->magenta==2.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.2->magenta==2.1.0) (2.21)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->magenta==2.1.0) (1.14.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->magenta==2.1.0) (0.1.7)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->magenta==2.1.0) (0.8.10)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (1.3.5)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (2.1.0)\n",
            "Collecting note-seq\n",
            "  Downloading note_seq-0.0.4-py3-none-any.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 88.2 MB/s \n",
            "\u001b[?25h  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 93.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (7.9.0)\n",
            "Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (2.3.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (22.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (6.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (6.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (2.11.3)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from intervaltree>=2.1.0->note-seq->magenta==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note-seq->magenta==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (5.7.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 83.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->IPython->note-seq->magenta==2.1.0) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->note-seq->magenta==2.1.0) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->IPython->note-seq->magenta==2.1.0) (0.7.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (4.6.0.66)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.1.4)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting gevent\n",
            "  Downloading gevent-22.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 71.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 92.4 MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow\n",
            "  Downloading mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 98.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (4.64.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.12.11)\n",
            "Collecting bz2file\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.7.1)\n",
            "Collecting pypng\n",
            "  Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (0.5.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 78.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dopamine-rl in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.0.5)\n",
            "Collecting tensorflow-probability\n",
            "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (3.1.0)\n",
            "Collecting kfac\n",
            "  Downloading kfac-0.2.4-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 94.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->tensor2tensor->magenta==2.1.0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->tensor2tensor->magenta==2.1.0) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->tensor2tensor->magenta==2.1.0) (3.11.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gevent->tensor2tensor->magenta==2.1.0) (2.0.1)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 88.4 MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.0) (3.0.1)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 73.5 MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 75.9 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 80.7 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 96.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->tensor2tensor->magenta==2.1.0) (1.2.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (0.28.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.9.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (3.2.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tensor2tensor->magenta==2.1.0) (2.7.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (2.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (0.10.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (5.10.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gan->tensor2tensor->magenta==2.1.0) (0.12.0)\n",
            "Building wheels for collected packages: dill, google-apitools, mir-eval, pretty-midi, python-rtmidi, docopt, bz2file\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=9e5bb471f9edddbccc066f906b5d71a6fb69d1e784fa5cc98a706e81e2a670a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131041 sha256=62643430fa22c406fc65cc7915e1a64085cb7428ba093cc2d0cebd5c79d31e37\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/54/79/85de1824f2f4175fb4960c72afb10045d86700c3941dc73685\n",
            "  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100720 sha256=9fcfc537ee6b23ce8ce591e53155415bfa39f31f48b81e7830e8fd6bc35c5445\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/53/83/1d50d15a666140d53eda589db005f7cb53b739c7e54711f51f\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591954 sha256=0f43b99c056db8a598703120a550c300346ab9bee8be59ba508835c2ff33d8d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/5a/e3/30eeb9a99350f3f7e21258fcb132743eef1a4f49b3505e76b6\n",
            "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp38-cp38-linux_x86_64.whl size=417625 sha256=d29c5120455eb8dea937c895f12f2165503bb10bad0311bb687e321a7119ec85\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/93/e9/7d805b982c4cb5c6cec3e77e1fc6e7417a193beca2230cea52\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=c51bcbd48bfe7700750a70ac7bdbb128b1b0227288d0abfe5a5ef98437f0a21b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6882 sha256=ef6805b3b61516ccd1d321be979ff50893c74eb78245bee5a6a506eadbc4c483\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/88/ce/c9430af242507ffff602cf86c5ff6a1ae5205cba5aaf21f6cc\n",
            "Successfully built dill google-apitools mir-eval pretty-midi python-rtmidi docopt bz2file\n",
            "Installing collected packages: cachetools, requests, llvmlite, numba, grpc-google-iam-v1, docopt, cloudpickle, zstandard, zope.interface, zope.event, tensorflow-probability, resampy, pymongo, overrides, orjson, objsize, mido, jedi, hdfs, google-cloud-pubsub, google-cloud-bigquery-storage, google-auth-httplib2, fasteners, fastavro, dill, tf-slim, tensorflow-gan, tensorflow-addons, pypng, pydub, pretty-midi, mesh-tensorflow, kfac, gunicorn, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsublite, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-apitools, gevent, bz2file, apache-beam, tensor2tensor, sox, sk-video, python-rtmidi, pygtrie, note-seq, mir-eval, dm-sonnet, magenta\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.2.0\n",
            "    Uninstalling cachetools-5.2.0:\n",
            "      Successfully uninstalled cachetools-5.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.17.0\n",
            "    Uninstalling tensorflow-probability-0.17.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.17.0\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.4.2\n",
            "    Uninstalling resampy-0.4.2:\n",
            "      Successfully uninstalled resampy-0.4.2\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.3\n",
            "    Uninstalling pymongo-4.3.3:\n",
            "      Successfully uninstalled pymongo-4.3.3\n",
            "  Attempting uninstall: google-cloud-bigquery-storage\n",
            "    Found existing installation: google-cloud-bigquery-storage 2.16.2\n",
            "    Uninstalling google-cloud-bigquery-storage-2.16.2:\n",
            "      Successfully uninstalled google-cloud-bigquery-storage-2.16.2\n",
            "  Attempting uninstall: google-auth-httplib2\n",
            "    Found existing installation: google-auth-httplib2 0.0.4\n",
            "    Uninstalling google-auth-httplib2-0.0.4:\n",
            "      Successfully uninstalled google-auth-httplib2-0.0.4\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 2.6.1\n",
            "    Uninstalling google-cloud-language-2.6.1:\n",
            "      Successfully uninstalled google-cloud-language-2.6.1\n",
            "  Attempting uninstall: google-cloud-datastore\n",
            "    Found existing installation: google-cloud-datastore 2.9.0\n",
            "    Uninstalling google-cloud-datastore-2.9.0:\n",
            "      Successfully uninstalled google-cloud-datastore-2.9.0\n",
            "Successfully installed apache-beam-2.43.0 bz2file-0.98 cachetools-4.2.4 cloudpickle-2.2.0 dill-0.3.1.1 dm-sonnet-2.0.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 gevent-22.10.2 google-apitools-0.5.31 google-auth-httplib2-0.1.0 google-cloud-bigquery-storage-2.13.2 google-cloud-bigtable-1.7.3 google-cloud-datastore-1.15.5 google-cloud-dlp-3.9.2 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.11 google-cloud-pubsublite-1.6.0 google-cloud-recommendations-ai-0.7.1 google-cloud-spanner-3.26.0 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 grpc-google-iam-v1-0.12.4 gunicorn-20.1.0 hdfs-2.7.0 jedi-0.18.2 kfac-0.2.0 llvmlite-0.32.1 magenta-2.1.0 mesh-tensorflow-0.1.21 mido-1.2.6 mir-eval-0.7 note-seq-0.0.3 numba-0.49.1 objsize-0.5.2 orjson-3.8.3 overrides-6.5.0 pretty-midi-0.2.9 pydub-0.25.1 pygtrie-2.5.0 pymongo-3.13.0 pypng-0.20220715.0 python-rtmidi-1.1.2 requests-2.28.1 resampy-0.3.1 sk-video-1.1.10 sox-1.4.1 tensor2tensor-1.15.7 tensorflow-addons-0.19.0 tensorflow-gan-2.1.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.6 zope.interface-5.5.2 zstandard-0.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 셀 실행 후 런타임 다시 시작 \n",
        "!pip install magenta==2.1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmafz-Ue1Plv"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library"
      ],
      "metadata": {
        "id": "NOVSa5BEEFp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlxPNpIYWr6Q",
        "outputId": "96a4cf85-2de8-4e64-ad8d-6b6df2ad00e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import zipfile \n",
        "import os\n",
        "import pandas as pd\n",
        "import IPython\n",
        "import collections\n",
        "import note_seq \n",
        "\n",
        "from magenta.common import merge_hparams\n",
        "from magenta.contrib import training as contrib_training\n",
        "from magenta.models.music_vae import MusicVAE\n",
        "from magenta.models.music_vae import lstm_models\n",
        "from magenta.models.music_vae import data\n",
        "from magenta.scripts.convert_dir_to_note_sequences import convert_directory , convert_midi \n",
        "from magenta.models.music_vae import configs\n",
        "from magenta.models.music_vae.trained_model import TrainedModel\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download data"
      ],
      "metadata": {
        "id": "lsuYQ4syEMFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1TVJgOECJ-f_a1AK8HNfB9GDXxY0ssQcr"
      ],
      "metadata": {
        "id": "FV0dsuRaELaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make directory \n",
        "!mkdir data_dir"
      ],
      "metadata": {
        "id": "pT757D2DERd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4x-frXAE6SA"
      },
      "outputs": [],
      "source": [
        "# zip파일 압축해제\n",
        "with zipfile.ZipFile('./groove-v1.0.0-midionly.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data_dir')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RttMxNHNWMr"
      },
      "outputs": [],
      "source": [
        "#경로지정 - convert_directory\n",
        "\n",
        "# 압축해제된 데이터 경로 \n",
        "root_dir = '/content/data_dir/groove' \n",
        "# MIDI 파일을 TFrecord로 변환한 파일 경로 \n",
        "output_file = '/content/data_dir/sequence.tfrecord'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "tnWy-YP91Plw",
        "outputId": "7b1869bb-767e-4d7a-e8bc-ab9b956ce062"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    drummer                session                        id          style  \\\n",
              "0  drummer1  drummer1/eval_session   drummer1/eval_session/1   funk/groove1   \n",
              "1  drummer1  drummer1/eval_session  drummer1/eval_session/10  soul/groove10   \n",
              "2  drummer1  drummer1/eval_session   drummer1/eval_session/2   funk/groove2   \n",
              "\n",
              "   bpm beat_type time_signature  \\\n",
              "0  138      beat            4-4   \n",
              "1  102      beat            4-4   \n",
              "2  105      beat            4-4   \n",
              "\n",
              "                                       midi_filename  \\\n",
              "0  drummer1/eval_session/1_funk-groove1_138_beat_...   \n",
              "1  drummer1/eval_session/10_soul-groove10_102_bea...   \n",
              "2  drummer1/eval_session/2_funk-groove2_105_beat_...   \n",
              "\n",
              "                                      audio_filename   duration split  \n",
              "0  drummer1/eval_session/1_funk-groove1_138_beat_...  27.872308  test  \n",
              "1  drummer1/eval_session/10_soul-groove10_102_bea...  37.691158  test  \n",
              "2  drummer1/eval_session/2_funk-groove2_105_beat_...  36.351218  test  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9494396a-d299-49e5-a62d-11c65e0bbb70\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>drummer</th>\n",
              "      <th>session</th>\n",
              "      <th>id</th>\n",
              "      <th>style</th>\n",
              "      <th>bpm</th>\n",
              "      <th>beat_type</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>midi_filename</th>\n",
              "      <th>audio_filename</th>\n",
              "      <th>duration</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/1</td>\n",
              "      <td>funk/groove1</td>\n",
              "      <td>138</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
              "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
              "      <td>27.872308</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/10</td>\n",
              "      <td>soul/groove10</td>\n",
              "      <td>102</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
              "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
              "      <td>37.691158</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/2</td>\n",
              "      <td>funk/groove2</td>\n",
              "      <td>105</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
              "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
              "      <td>36.351218</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9494396a-d299-49e5-a62d-11c65e0bbb70')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9494396a-d299-49e5-a62d-11c65e0bbb70 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9494396a-d299-49e5-a62d-11c65e0bbb70');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# MIDI파일 정보 확인 \n",
        "df = pd.read_csv('/content/groove/info.csv')\n",
        "df = pd.DataFrame(df)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFLY9qnB1Plx"
      },
      "source": [
        "## Convert MIDI Directory to TFrecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLsHN1bpXHE8",
        "outputId": "70c20299-9539-429c-d705-785123dfba4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/README\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/info.csv\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/LICENSE\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer9/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer9/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer10/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer10/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer3/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer3/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer4/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer4/session1/Icon\n"
          ]
        }
      ],
      "source": [
        "# == magenta/scripts/convert_dir_to_note_sequences.py == \n",
        "# == convert_directory 사용 == 디렉토리 전체를 변환해주는 함수 \n",
        "\n",
        "convert_directory(data_root,tfrec_root,recursive=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S2SMmhn1Ply"
      },
      "source": [
        "# Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmRHxnpYbxgo"
      },
      "outputs": [],
      "source": [
        "# == magenta/models/music_vae/configs.py == \n",
        "\n",
        "class Config(collections.namedtuple(\n",
        "    'Config',\n",
        "    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
        "     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n",
        "\n",
        "    def values(self):\n",
        "        return self._asdict()\n",
        "\n",
        "Config.__new__.__defaults__ = (None,) * len(Config._fields)\n",
        "\n",
        "\n",
        "def update_config(config, update_dict):\n",
        "    config_dict = config.values()\n",
        "    config_dict.update(update_dict)\n",
        "    return Config(**config_dict)\n",
        "\n",
        "\n",
        "CONFIG_MAP = {}\n",
        "\n",
        "\n",
        "HParams = contrib_training.HParams\n",
        "\n",
        "\n",
        "# GrooVAE configs 중 4마디를 추출하기 위한 'groovae_4bar' default로 가지고 옴\n",
        "\n",
        "CONFIG_MAP['groovae_4bar'] = Config(\n",
        "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(), # BidirectionalLstmEncoder \n",
        "                   lstm_models.GrooveLstmDecoder()), # Groove LSTM Decoder with MSE loss for continuous values\n",
        "    hparams=merge_hparams(\n",
        "        lstm_models.get_default_hparams(),\n",
        "        HParams(\n",
        "            batch_size=512, # Minibatch size\n",
        "            max_seq_len=16 * 4,  # 4 bars with 16 steps per bar\n",
        "            z_size=256, # Size of latent vector\n",
        "            enc_rnn_size=[512], # Size of Encoder\n",
        "            dec_rnn_size=[256, 256],# Size of decoder\n",
        "            max_beta=0.2,\n",
        "            free_bits=48,\n",
        "            dropout_keep_prob=0.3,\n",
        "        )),\n",
        "    note_sequence_augmenter=None,\n",
        "    data_converter=data.GrooveConverter(\n",
        "        split_bars=4, steps_per_quarter=4, quarters_per_bar=4,\n",
        "        max_tensors_per_notesequence=20,\n",
        "        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n",
        "\n",
        "        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n",
        "   # train data 경로 설정 \n",
        "    train_examples_path='/content/data_dir/sequence.tfrecord', \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vJu1kLTSGUt"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUoVCmFfynAy"
      },
      "outputs": [],
      "source": [
        "#============ License=====================================================\n",
        "# Copyright 2022 The Magenta Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "#=======================================================================\n",
        "\n",
        "#=== MusicVAE train script===\n",
        "#=== magenta/models/music_vae/music_vae_train.py ===\n",
        "\n",
        "# Should not be called from within the graph to avoid redundant summaries.\n",
        "def _trial_summary(hparams, examples_path, output_dir):\n",
        "  #=== Writes a tensorboard text summary of the trial ===\n",
        "\n",
        "  examples_path_summary = tf.summary.text(\n",
        "      'examples_path', tf.constant(examples_path, name='examples_path'),\n",
        "      collections=[])\n",
        "\n",
        "  hparams_dict = hparams.values()\n",
        "\n",
        "#=== Create a markdown table from hparams.===\n",
        "\n",
        "  header = '| Key | Value |\\n| :--- | :--- |\\n'\n",
        "  keys = sorted(hparams_dict.keys())\n",
        "  lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n",
        "  hparams_table = header + '\\n'.join(lines) + '\\n'\n",
        "\n",
        "  hparam_summary = tf.summary.text(\n",
        "    'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n",
        "        writer.add_summary(examples_path_summary.eval())\n",
        "        writer.add_summary(hparam_summary.eval())\n",
        "        writer.close()\n",
        "\n",
        "\n",
        "def _get_input_tensors(dataset, config):\n",
        "  #== Get input tensors from dataset ===\n",
        "    batch_size = config.hparams.batch_size\n",
        "    iterator = tf.data.make_one_shot_iterator(dataset)\n",
        "    (input_sequence, output_sequence, control_sequence,\n",
        "    sequence_length) = iterator.get_next()\n",
        "    input_sequence.set_shape(\n",
        "    [batch_size, None, config.data_converter.input_depth])\n",
        "    output_sequence.set_shape(\n",
        "    [batch_size, None, config.data_converter.output_depth])\n",
        "    \n",
        "    if not config.data_converter.control_depth:\n",
        "        control_sequence = None\n",
        "    \n",
        "    else:\n",
        "        control_sequence.set_shape(\n",
        "            [batch_size, None, config.data_converter.control_depth])\n",
        "        sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
        "        \n",
        "    return {\n",
        "        'input_sequence': input_sequence,\n",
        "        'output_sequence': output_sequence,\n",
        "        'control_sequence': control_sequence,\n",
        "        'sequence_length': sequence_length\n",
        "    }\n",
        "\n",
        "#=== Set Training checkpoints and hours  ===\n",
        "def train(train_dir,\n",
        "          config,\n",
        "          dataset_fn,\n",
        "          checkpoints_to_keep=5,\n",
        "          keep_checkpoint_every_n_hours=1,\n",
        "          num_steps=None,\n",
        "          master='',\n",
        "          num_sync_workers=0,\n",
        "          num_ps_tasks=0,\n",
        "          task=0):\n",
        "\n",
        "#====  Train loop ====\n",
        "    tf.gfile.MakeDirs(train_dir)\n",
        "    is_chief = (task == 0)\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.device(tf.train.replica_device_setter(\n",
        "            num_ps_tasks, merge_devices=True)):\n",
        "            \n",
        "            model = config.model\n",
        "            model.build(config.hparams,\n",
        "                        config.data_converter.output_depth,\n",
        "                        is_training=True)\n",
        "            #== Optimizer ==\n",
        "            optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
        "\n",
        "            hooks = []\n",
        "            if num_sync_workers:\n",
        "                optimizer = tf.train.SyncReplicasOptimizer(\n",
        "                    optimizer,num_sync_workers)\n",
        "                hooks.append(optimizer.make_session_run_hook(is_chief))\n",
        "\n",
        "            grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
        "            global_norm = tf.global_norm(grads)\n",
        "            tf.summary.scalar('global_norm', global_norm)\n",
        "            \n",
        "            if config.hparams.clip_mode == 'value':\n",
        "                g = config.hparams.grad_clip\n",
        "                clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
        "            elif config.hparams.clip_mode == 'global_norm':\n",
        "                clipped_grads = tf.cond(\n",
        "                    global_norm < config.hparams.grad_norm_clip_to_zero,\n",
        "                    lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
        "                        grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
        "                    lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
        "                train_op = optimizer.apply_gradients(\n",
        "                    list(zip(clipped_grads, var_list)),\n",
        "                    global_step=model.global_step,\n",
        "                    name='train_step')\n",
        "            logging_dict = {'global_step': model.global_step,\n",
        "                            'loss': model.loss}\n",
        "            \n",
        "            hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
        "            if num_steps:\n",
        "                hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
        "                \n",
        "            scaffold = tf.train.Scaffold(\n",
        "                saver=tf.train.Saver(\n",
        "                    max_to_keep=checkpoints_to_keep,\n",
        "                    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
        "            \n",
        "            tf_slim.training.train(\n",
        "                train_op=optimizer.apply_gradients(\n",
        "                    list(zip(clipped_grads, var_list)),\n",
        "                    global_step=model.global_step,\n",
        "                    name='train_step'),\n",
        "                logdir=train_dir,\n",
        "                scaffold=scaffold,\n",
        "                hooks=hooks,\n",
        "                save_checkpoint_secs=60,\n",
        "                master=master,\n",
        "                is_chief=is_chief)\n",
        "\n",
        "\n",
        "def run(config_map,\n",
        "        tf_file_reader=tf.data.TFRecordDataset,\n",
        "        file_reader=tf.python_io.tf_record_iterator,\n",
        "        is_training=True):\n",
        "    # Load model params, save config file and start trainer\n",
        "    config = config_map['groovae_4bar']\n",
        "    train_dir = '/content/train'\n",
        "    num_steps = 50000  #epoch\n",
        "    \n",
        "    def dataset_fn():\n",
        "        return data.get_dataset(\n",
        "            config,\n",
        "            tf_file_reader=tf_file_reader,\n",
        "            is_training=True,\n",
        "            cache_dataset=True)\n",
        "    \n",
        "    if is_training == True:\n",
        "        train(\n",
        "            train_dir,\n",
        "            config=config,\n",
        "            dataset_fn=dataset_fn,\n",
        "            num_steps=num_steps)      \n",
        "    \n",
        "    else:\n",
        "        print(\"Training is False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ri88cYDTRT_",
        "outputId": "a1e42c4d-8830-40e1-ebef-46cb54f94ba4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py:1066: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        }
      ],
      "source": [
        "run(CONFIG_MAP) #epoch 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyE-Cw-MWWtH"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir gen_midi"
      ],
      "metadata": {
        "id": "pu9L80sdSnFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCVn7uomgeWZ",
        "outputId": "122a6e9f-dcac-4f3d-ce13-c3f25de570dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        }
      ],
      "source": [
        "#=== magenta/magenta/models/music_vae/music_vae_generate.py ===\n",
        "model = TrainedModel(\n",
        "    config=CONFIG_MAP['groovae_4bar'],\n",
        "    batch_size=1,\n",
        "    checkpoint_dir_or_path='/content/train') # 체크포인트의 경로\n",
        "\n",
        "generated_sequence = model.sample(n=1, length=16*4, temperature=0.5)\n",
        "note_seq.sequence_proto_to_midi_file(generated_sequence[0], '/content/gen_midi/drum_4bar.mid')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}